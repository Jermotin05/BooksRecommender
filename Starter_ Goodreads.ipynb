{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Analysis\nTo begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\n#import isbnlib\n#from newspaper import Article\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n#from tqdm import tqdm\nimport re\nfrom scipy.cluster.vq import kmeans, vq\nfrom pylab import plot, show\nfrom matplotlib.lines import Line2D\nimport matplotlib.colors as mcolors\n#import goodreads_api_client as gr\nfrom sklearn.cluster import KMeans\nfrom sklearn import neighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom mpl_toolkits.mplot3d import Axes3D","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/books.csv', error_bad_lines = False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you're ready to read in the data and use the plotting functions to visualize the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Let's check 1st file: ../input/books.csv","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/books.csv', error_bad_lines = False)\ndf1.dataframeName = 'books.csv'\ndf1.index = df1['bookID']\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df1.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.replace(to_replace='J.K. Rowling/Mary GrandPr√©', value = 'J.K. Rowling', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Columns Description: \n\n- **bookID** Contains the unique ID for each book/series\n- **title** contains the titles of the books\n- **authors** contains the author of the particular book\n- **average_rating** the average rating of the books, as decided by the users\n- **ISBN** ISBN(10) number, tells the information about a book - such as edition and publisher\n- **ISBN 13** The new format for ISBN, implemented in 2007. 13 digits\n- **language_code** Tells the language for the books\n- **Num_pages** Contains the number of pages for the book\n- **Ratings_count** Contains the number of ratings given for the book\n- **text_reviews_count** Has the count of reviews left by users","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis<a id=\"3\"></a> <br>","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"sns.set_context('poster')\nplt.figure(figsize=(50,15))\nbooks = df1['title'].value_counts()[:30]\nrating = df1.average_rating[:30]\nsns.barplot(x = books, y = books.index, palette='rocket')\nplt.title(\"Most Occurring Books\")\nplt.xlabel(\"Number of occurances\")\nplt.ylabel(\"Books\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like older books seem to show up more often. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Correlation matrix:","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter and density plots:","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotScatterMatrix(df1, 10, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_rated = df1.sort_values('ratings_count', ascending = False).head(45).set_index('title')\nplt.figure(figsize=(20,20))\nsns.barplot(most_rated['ratings_count'], most_rated.index, palette='rocket')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* *This data is intersting because it show that typically the top rated books are part of a series.\n* *Whats even more intersting is that while series dominate the top ratings count,there are also a few instances where only the first book in the  series was enjoyed and after that the rest of the series tanks, so basically a one hit wonder\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"most_books = df1.groupby('authors')['title'].count().reset_index().sort_values('title', ascending=False).head(10).set_index('authors')\nmost_books.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fid top aurthors\nsns.set_context('talk')\nmost_books = df1.groupby('authors')['title'].count().reset_index().sort_values('title', ascending=False).head(10).set_index('authors')\nplt.figure(figsize=(15,10))\nax = sns.barplot(most_books['title'], most_books.index, palette='icefire_r')\nax.set_title(\"Top 10 authors with most books\")\nax.set_xlabel(\"Total number of books\")\nfor i in ax.patches:\n    ax.text(i.get_width()+.3, i.get_y()+0.5, str(round(i.get_width())), fontsize = 10, color = 'k')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"high_rated_author = df1[df1['average_rating']>=4.3]\nhigh_rated_author = high_rated_author.groupby('authors')['title'].count().reset_index().sort_values('title', ascending = False).head(10).set_index('authors')\nplt.figure(figsize=(15,10))\nax = sns.barplot(high_rated_author['title'], high_rated_author.index, palette='Set2')\nax.set_xlabel(\"Number of Books\")\nax.set_ylabel(\"Authors\")\nfor i in ax.patches:\n    ax.text(i.get_width()+.3, i.get_y()+0.5, str(round(i.get_width())), fontsize = 10, color = 'k')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def segregation(data):\n    values = []\n    for val in data.average_rating:\n        if val>=0 and val<=1:\n            values.append(\"Between 0 and 1\")\n        elif val>1 and val<=2:\n            values.append(\"Between 1 and 2\")\n        elif val>2 and val<=3:\n            values.append(\"Between 2 and 3\")\n        elif val>3 and val<=4:\n            values.append(\"Between 3 and 4\")\n        elif val>4 and val<=5:\n            values.append(\"Between 4 and 5\")\n        else:\n            values.append(\"NaN\")\n    print(len(values))\n    return values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.average_rating.isnull().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.dropna(0, inplace=True)\n#Removing Any null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nrating= df1.average_rating.astype(float)\nsns.distplot(rating, bins=20)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the given plot, we can infer that: \n\n- Majority of the ratings lie near 3.7-4.3, approximately.\n- Books having scores near 5 are extremely rare","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for any relation between ratings and review counts\nplt.figure(figsize=(15,10))\ndf1.dropna(0, inplace=True)\nsns.set_context('paper')\nax =sns.jointplot(x=\"average_rating\",y='text_reviews_count', kind='scatter',  data= df1[['text_reviews_count', 'average_rating']])\nax.set_axis_labels(\"Average Rating\", \"Text Review Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysis: We can infer from the plot that most of the ratings for the books seems to lie near 3-4, with a heavy amount of reviews lying barely near 5000, approximately. Let's plot that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trial = df1[~(df1['text_reviews_count']>5000)]\ntrial.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for any relation between them.\nplt.figure(figsize=(15,10))\ndf1.dropna(0, inplace=True)\nsns.set_context('paper')\nax =sns.jointplot(x=\"average_rating\",y='text_reviews_count', kind='scatter',  data= trial, color = 'green')\nax.set_axis_labels(\"Average Rating\", \"Text Review Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shows majority of ratings are still where text-review count is under 1k","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking relationship between numberofpages and ratings\nplt.figure(figsize=(15,10))\nsns.set_context('paper')\nax = sns.jointplot(x=\"average_rating\", y=\"  num_pages\", data = df1, color = 'crimson')\nax.set_axis_labels(\"Average Rating\", \"Number of Pages\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much info here need to take a closer look and only look where num_pages is less than 1k","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trial = df1[~(df1['  num_pages']>1000)]\ntrial.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.jointplot(x=\"average_rating\", y=\"  num_pages\", data = trial, color = 'darkcyan')\nax.set_axis_labels(\"Average Rating\", \"Number of Pages\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems the best and worst books are found in the 150-400 range of page numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check relationship between ratings and ratings count\nsns.set_context('paper')\nax = sns.jointplot(x=\"average_rating\", y=\"ratings_count\", data = df1, color = 'orange')\nax.set_axis_labels(\"Average Rating\", \"Ratings Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get rid of outliers and lets look closer where ratings cound is less than 20,000,000\ntrial = df1[~(df1.ratings_count>2000000)]\ntrial.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context('paper')\nax = sns.jointplot(x=\"average_rating\", y=\"ratings_count\", data = trial, color = 'brown')\nax.set_axis_labels(\"Average Rating\", \"Ratings Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph, we can see that there can be a potential relationship between the average rating and ratings count. As the number of ratings increase, the rating for the book seems to taper towards 4. The average rating seems to become sparse while the number keeps on decreasing. We can also notice that a 5 star rating seems to only happen with a small ratings count","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# K Means Clustering\n\n* KMeans clustering is a type of unsupervised learning which groups unlabelled data. The goal is to find groups in data.\n\n* Here I want to find natural clusters between the rating count and average rating value.\n\n\n\n\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trial = df1[['average_rating', 'ratings_count','  num_pages','text_reviews_count']]\ndata = np.asarray([np.asarray(trial['average_rating']), np.asarray(trial['ratings_count'])]).T\ntrial.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Determine how many clusters K \n* Find K using scree plot/elbow curve method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data\ndistortions = []\nfor k in range(2,20):\n    k_means = KMeans(n_clusters = k)\n    k_means.fit(X)\n    distortions.append(k_means.inertia_)\n\nfig = plt.figure(figsize=(15,10))\nplt.plot(range(2,20), distortions, 'bx-')\nplt.title(\"Elbow Curve\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that elbow is around 5-7 so we can start with 5 clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Computing K means with K = 5, thus, taking it as 5 clusters\ncentroids, _ = kmeans(data, 5)\n\n#assigning each sample to a cluster\n#Vector Quantisation:\n\nidx, _ = vq(data, centroids)\nidx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some plotting using numpy's logical indexing\nsns.set_context('paper')\nplt.figure(figsize=(15,10))\nplt.plot(data[idx==0,0],data[idx==0,1],'or',#red circles\n     data[idx==1,0],data[idx==1,1],'ob',#blue circles\n     data[idx==2,0],data[idx==2,1],'oy', #yellow circles\n     data[idx==3,0],data[idx==3,1],'om', #magenta circles\n     data[idx==4,0],data[idx==4,1],'ok',#black circles\n    \n     \n        \n        \n        \n        \n        )\nplt.plot(centroids[:,0],centroids[:,1],'sg',markersize=8, )\n\n\n\n\ncircle1 = Line2D(range(1), range(1), color = 'red', linewidth = 0, marker= 'o', markerfacecolor='red')\ncircle2 = Line2D(range(1), range(1), color = 'blue', linewidth = 0,marker= 'o', markerfacecolor='blue')\ncircle3 = Line2D(range(1), range(1), color = 'yellow',linewidth=0,  marker= 'o', markerfacecolor='yellow')\ncircle4 = Line2D(range(1), range(1), color = 'magenta', linewidth=0,marker= 'o', markerfacecolor='magenta')\ncircle5 = Line2D(range(1), range(1), color = 'black', linewidth = 0,marker= 'o', markerfacecolor='black')\n\nplt.legend((circle1, circle2, circle3, circle4, circle5)\n           , ('Cluster 1','Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5'), numpoints = 1, loc = 0, )\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Remove some outliers to get a more accurate model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the outliers\ntrial.idxmax()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trial.drop(2034, inplace = True)\ntrial.drop(41865, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.asarray([np.asarray(trial['average_rating']), np.asarray(trial['ratings_count'])]).T\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids, _ = kmeans(data, 5)\n\n#assigning each sample to a cluster\n#Vector Quantisation:\n\nidx, _ = vq(data, centroids)\nidx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some plotting using numpy's logical indexing\nsns.set_context('paper')\nplt.figure(figsize=(15,10))\nplt.plot(data[idx==0,0],data[idx==0,1],'or',#red circles\n     data[idx==1,0],data[idx==1,1],'ob',#blue circles\n     data[idx==2,0],data[idx==2,1],'oy', #yellow circles\n     data[idx==3,0],data[idx==3,1],'om', #magenta circles\n     data[idx==4,0],data[idx==4,1],'ok',#black circles\n    \n     \n        \n        \n        \n        \n        )\nplt.plot(centroids[:,0],centroids[:,1],'sg',markersize=8, )\n\n\n\n\ncircle1 = Line2D(range(1), range(1), color = 'red', linewidth = 0, marker= 'o', markerfacecolor='red')\ncircle2 = Line2D(range(1), range(1), color = 'blue', linewidth = 0,marker= 'o', markerfacecolor='blue')\ncircle3 = Line2D(range(1), range(1), color = 'yellow',linewidth=0,  marker= 'o', markerfacecolor='yellow')\ncircle4 = Line2D(range(1), range(1), color = 'magenta', linewidth=0,marker= 'o', markerfacecolor='magenta')\ncircle5 = Line2D(range(1), range(1), color = 'black', linewidth = 0,marker= 'o', markerfacecolor='black')\n\nplt.legend((circle1, circle2, circle3, circle4, circle5)\n           , ('Cluster 1','Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5'), numpoints = 1, loc = 0, )\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, now we can see that once the whole system _can_ be classified into clusters. As the count increases, the rating would end up near the cluster given above. The green squares are the centroids for the given clusters. \n\nAs the rating count seems to decrease, the average rating seems to become sparser, with higher volatility and less accuracy. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Making Reccomendations\n\nHaving seen the clustering, we can infer that there can be some recommendations which can happen with the relation between Average Rating and Ratings Count. \n\nWe can create a reccomendation algorithm using K Nearest Neighbors using the ratings distribution\n\nBased on a book entered by the user, the nearest neighbours to it would be classified as the books which the user might like. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"One of the first things that need to happen int he creation of a books feature table, this table is simply a binning of the books ratings such as\n- Between 0 and 1\n- Between 1 and 2\n- Between 2 and 3\n- Between 3 and 4\n- Between 4 and 5\n\nThe recommendations then consider the average ratings and ratings count for the query entered.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Ratings_Dist'] = segregation(df1)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert the categorical ratings distribution to indicator variables\nbooks_features = pd.concat([df1['Ratings_Dist'].str.get_dummies(sep=\",\"), df1['average_rating'], df1['ratings_count']], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"books_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The min-max scaler is used to reduce the bias which would have been present due to some books having a massive amount of features, yet the rest having less. Min-Max scaler would find the median for them all and equalize it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_scaler = MinMaxScaler()\nbooks_features = min_max_scaler.fit_transform(books_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.round(books_features,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = neighbors.NearestNeighbors(n_neighbors=7, algorithm='auto')\nmodel.fit(books_features)\ndistance, indices = model.kneighbors(books_features)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating specific functions to help in finding the book names: \n- Get index from Title\n- Get ID from partial name\n- Print the similar books from the feature dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_index_from_name(name):\n    return df1[df1[\"title\"]==name].index.tolist()[0]\n\nall_books_names = list(df1.title.values)\n\ndef get_id_from_partial_name(partial):\n    for name in all_books_names:\n        if partial in name:\n            print(name,all_books_names.index(name))\n            \ndef print_similar_books(query=None,id=None):\n    if id:\n          for idx,id in enumerate(indices[id][1:]):\n            print(df1.iloc[id][\"title\"] ,\" [ID: {bid}] - [Distance: {dis}]\".format(bid=df1.iloc[id][\"bookID\"],dis=distance[id][idx]))\n    if query:\n        found_id = get_index_from_name(query)\n        for idx,id in enumerate(indices[found_id][1:]):\n            print(df1.iloc[id][\"title\"] ,\" [ID: {bid}] - [Distance: {dis}]\".format(bid=df1.iloc[id][\"bookID\"],dis=distance[id][idx]))\n            \n# This will return a user score based off a rating the user gave the book and the distance of how closely related the books are to one another, the smaller the user score the moore liekly the user will enjoy the book\ndef get_similar_userRated_books(query=None,id=None,user_rating=1):\n    userData = []\n    if id:\n        for idx,id in enumerate(indices[id][1:]):\n            bookID = df1.iloc[id][\"bookID\"]\n            userScore = distance[id][idx] / user_rating\n            data  = (bookID,userScore)\n            print(df1.iloc[id][\"title\"] ,\" [ID: {bid}] - [Distance: {dis}]\".format(bid=df1.iloc[id][\"bookID\"],dis=distance[id][idx]))\n            print(\"id: {}  -  score: {}\".format(bookID,userScore))            \n            userData.append(data)            \n    if query:\n        found_id = get_index_from_name(query)\n        for idx,id in enumerate(indices[found_id][1:]):\n            bookID = df1.iloc[id][\"bookID\"]\n            userScore = distance[id][idx] / user_rating\n            print(df1.iloc[id][\"title\"] ,\" [ID: {bid}] - [Distance: {dis}]\".format(bid=df1.iloc[id][\"bookID\"],dis=distance[id][idx]))\n            print(\"id: {}  -  score: {}\".format(bookID,userScore))            \n            userData.append(data)\n    print(userData)\n    return userData\n            \ndef get_similar_userRated_books_from_list(userRateData):\n    userData = []\n    for (bookID,score) in userRateData:\n        for idx,id in enumerate(indices[bookID][1:]):\n            bookID = df1.iloc[id][\"bookID\"]\n            userScore = distance[id][idx] / score            \n            data  = (bookID,userScore)            \n            userData.append(data)      \n            sortedData = sorted(userData,key=lambda tup:tup[1])\n    get_books_from_userScore(sortedData)\n\ndef get_books_from_userScore(userData):    \n    data = []\n    for (bookID,score) in userData:\n        print(bookID,\":\",df1.loc[bookID].title,\" - \",score)\n        data.append((bookID,df1.loc[bookID].title,score))\n        # return the list of tuples containing id,title,and score all already ordered\n    return data\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_similar_books(\"Harry Potter and the Half-Blood Prince (Harry Potter  #6)\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_userRated_books(id=1,user_rating=3.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_similar_books(id=5107)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_id_from_partial_name(\"Percy J\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample Method Calls for API","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print_similar_books(\"Harry Potter and the Half-Blood Prince (Harry Potter  #6)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mock user bookid,score tuple, this will be all we need to get the overall reccommendation list for the user\ndata = [(3,5),(2,3.5),(753,2),(87,4.2)]\nuserData =get_similar_userRated_books_from_list(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}